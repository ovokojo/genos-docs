{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Demo\n",
    "---\n",
    "<br>\n",
    "\n",
    "## Install dependencies\n",
    "\n",
    "1. Do the following before installing the dependencies found in `requirements.txt` file because of current challenges installing `onnxruntime` through `pip install onnxruntime`. \n",
    "\n",
    "    - For MacOS users, a workaround is to first install `onnxruntime` dependency for `chromadb` using:\n",
    "\n",
    "    ```python\n",
    "     conda install onnxruntime -c conda-forge\n",
    "    ```\n",
    "    See this [thread](https://github.com/microsoft/onnxruntime/issues/11037) for additonal help if needed. \n",
    "\n",
    "     - For Windows users, follow the guide [here](https://github.com/bycloudai/InstallVSBuildToolsWindows?tab=readme-ov-file) to install the Microsoft C++ Build Tools. Be sure to follow through to the last step to set the enviroment variable path.\n",
    "\n",
    "\n",
    "2. Now run this command to install dependenies in the `requirements.txt` file. \n",
    "\n",
    "```python\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "3. Install markdown depenendies with: \n",
    "\n",
    "```python\n",
    "pip install \"unstructured[md]\"\n",
    "```\n",
    "\n",
    "You'll also need to set up an OpenAI account (and set the OpenAI key in your environment variable) for this to work.\n",
    "<br>\n",
    "\n",
    "### References\n",
    "\n",
    "Instructions and code snippets from [Langchain RAG Tutorial](https://github.com/pixegami/langchain-rag-tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"unstructured[md]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create database\n",
    "\n",
    "Create the Chroma DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import stat\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data\"\n",
    "\n",
    "# Check and clear the database directory\n",
    "if os.path.exists(CHROMA_PATH):\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "    print(f\"Cleared the existing database at {CHROMA_PATH}.\")\n",
    "\n",
    "os.makedirs(CHROMA_PATH, exist_ok=True)\n",
    "os.chmod(CHROMA_PATH, stat.S_IWUSR | stat.S_IREAD)\n",
    "\n",
    "# Load documents\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "documents = load_documents()\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "\n",
    "# Split text into chunks\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=400,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(documents)\n",
    "\n",
    "# Save to Chroma vector store\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "\n",
    "save_to_chroma(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "\n",
    "# Use a temporary directory for Chroma to avoid permission issues\n",
    "CHROMA_PATH = tempfile.mkdtemp()\n",
    "print(f\"Using temporary directory for Chroma: {CHROMA_PATH}\")\n",
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "\n",
    "documents = load_documents()\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "chunks = split_text(documents)\n",
    "save_to_chroma(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the database\n",
    "\n",
    "Query the Chroma DB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "def main(query_text):\n",
    "    # Prepare the DB.\n",
    "    embedding_function = OpenAIEmbeddings()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "    if len(results) == 0 or results[0][1] < 0.7:\n",
    "        print(f\"Unable to find matching results.\")\n",
    "        return\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    print(prompt)\n",
    "\n",
    "    model = ChatOpenAI()\n",
    "    response_text = model.predict(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "\n",
    "# Example usage\n",
    "query_text = input(\"Enter the query text: \")\n",
    "main(query_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
